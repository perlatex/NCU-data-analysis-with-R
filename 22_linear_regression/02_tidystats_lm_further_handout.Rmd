---
title: "线性回归 2"
author: "王敏杰"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    latex_engine: xelatex
    number_sections: yes
    df_print: kable
linkcolor: red
urlcolor: red
header-includes:
  - \usepackage[fontset = fandol]{ctex}
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{underscore}
  - \usepackage{booktabs}
#  - \usepackage{indentfirst}\setlength{\parindent}{2em}
classoptions: "hyperref, 12pt, a4paper"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo      = TRUE,
  message   = FALSE,
  warning   = FALSE,
  out.width = '75%',
  fig.align = "center"
)
```



# 线性回归的前提假设

线性模型
$$
y_n = \alpha + \beta x_n + \epsilon_n \quad \text{where}\quad
\epsilon_n \sim \operatorname{normal}(0,\sigma).
$$

线性回归需要满足四个前提假设：

1. **Linearity **
    - 因变量和每个自变量都是线性关系
    
```{r, out.width = '80%', fig.align='center', echo = FALSE}
knitr::include_graphics(path = "images/jhudsl/Linearity1.png")
knitr::include_graphics(path = "images/jhudsl/Linearity2.png")
```


2. **Indpendence **
    - 对于所有的观测值，它们的误差项相互之间是独立的



3. **Normality **
    - 误差项服从正态分布

```{r, out.width = '80%', fig.align='center', echo = FALSE}
knitr::include_graphics(path = "images/jhudsl/Normality1.png")
knitr::include_graphics(path = "images/jhudsl/Normality2.png")
```



4. **Equal-variance **  
    - 所有的误差项具有同样方差
    
```{r, out.width = '80%', fig.align='center', echo = FALSE}
knitr::include_graphics(path = "images/jhudsl/Homoscedasticity.png")
```

这四个假设的首字母，合起来就是**LINE**，这样很好记


把这**四个前提**画在一张图中

```{r, out.width = '80%', fig.align='center', echo = FALSE}
knitr::include_graphics(path = "images/Normal-distributed-data.png")
```


**课堂练习**：以下是否违背了LINE假设

1. 努力学习与是否通过R语言考试？
    - *响应变量* 是否通过考试 (Pass or Fail)
    - *解释变量:* 课后练习时间 (in hours) 

2. 汽车音乐音量大小与司机刹车的反应时
    - *响应变量* 反应时
    - *解释变量:* 音量大小 



# 回到案例

```{r message = FALSE, warning = FALSE}
library(tidyverse)
wages <- read_csv("./demo_data/wages.csv")

wages %>% head()
```



$$
\begin{aligned}
y_{i}=\beta_{0}+\beta_{1}\textrm{height}_{i}+\epsilon_{i}\quad &\textrm{where} \quad \epsilon_{i}\sim \textrm{N}(0,\sigma^2) 
\end{aligned}
$$



```{r}
mod1 <- lm(
  formula = earn ~ 1 + height,
  data = wages
)

summary(mod1)
```




# 更多模型

模型只是一种探测手段，通过捕获数据特征，探测数据的产生机制。所以说，模型只是人的一种假设，或者人的期望，试图去解释现象，但真实的情况，可能不是这样，任何一种模型都不正确。

所以，遇到不符合预期的情况，是正常的。我们可以带入更多的解释变量，建立若干个模型，逐一尝试，然后从中选择一个最好模型。

但要记住，所有模型都是人的"意淫"，要正确看待模型结果。



## 增加解释变量

之前是用单个变量`height`预测`earn`，我们可以增加一个解释变量`edu`，稍微扩展一下我们的一元线性模型，就是多元回归模型

$$
\begin{aligned}
\text{earn} &= \beta_0 + \beta_1 \text{height} + \beta_2 \text{edu} +\epsilon \\
\end{aligned}
$$

R语言代码实现也很简单，只需要把变量`edu`增加在公式的右边
```{r}
mod2 <- lm(earn ~ 1 + height + edu, data = wages)
```

同样，我们打印`mod2`看看

```{r}
summary(mod2)
```



## 分类变量

我们将**身高**和**性别**同时考虑进模型


$$
\begin{aligned}
Y_{i}=\beta_{0}+\beta_{1}\textrm{height}_{i}+\beta_{2}\textrm{sex}_{i}+\epsilon_{i}\quad \textrm{where}\quad \epsilon_{i}\sim \textrm{N}(0,\sigma^2)
\end{aligned}
$$
此时预测变量是一个分类变量和一个连续变量

```{r}
mod5 <- lm(earn ~ 1 + height + sex, data = wages)
coef(mod5)
```

- `height = 879.424`  当sex保持不变时，height变化一个单位引起的earn变化
- `sexmale = 16874.158`  当height保持不变时，sex变化引起的earn变化(male与female的差值)


### 换种方式理解

$$
\begin{aligned}
Y_{i}=\beta_{0}+\beta_{1}\textrm{height}_{i}+\beta_{2}\textrm{sex}_{i}+\epsilon_{i}
\end{aligned}
$$


事实上，分类变量`sex`在R语言代码里，会转换成0和1这种**虚拟变量**，然后再计算。


我们这里显式地构建一个新变量`gender`，将`sex`中`(male, female)`替换成`(1, 0)`

```{r}
wages_gender <- wages %>% 
  mutate(gender = if_else(sex == "male", 1, 0))

wages_gender %>% head()
```


这样，我们获得一个等价的模型
$$
\begin{aligned}
y_{i}=\beta_{0}+\beta_{1}\textrm{height}_{i}+\beta_{2}\textrm{gender}_{i}+\epsilon_{i}
\end{aligned}
$$


然后放入`lm()`
```{r}
mod5a <- lm(earn ~ 1 + height + gender, data = wages_gender)
coef(mod5a)
```

我们发现系数没有发生变化，但更容易理解。

在固定的身高上，比较不同性别的收入**差异**:

- 当 `gender = 0` 情形

$$
\begin{aligned}
y_{i}&=\beta_{0}+\beta_{1}\textrm{height}_{i}+\beta_{2}\textrm{gender}_{i}+\epsilon_{i}\\
     &= \beta_{0}+ \beta_{1}\textrm{height}_{i} + \epsilon_{i}
\end{aligned}
$$

- 当 `gender = 1` 情形

$$
\begin{aligned}
y_{i}&=\beta_{0}+\beta_{1}\textrm{height}_{i}+\beta_{2}\textrm{gender}_{i}+\epsilon_{i}\\
     &= \beta_{0}+ \beta_{1}\textrm{height}_{i} + \beta_{2} + \epsilon_{i} \\
     &= (\beta_{0}+ \beta_{2}) + \beta_{1}\textrm{height}_{i}  + \epsilon_{i}
\end{aligned}
$$

男性和女性的两条拟合直线，斜率相同、截距不同。


```{r}
wages_gender %>%
  ggplot(aes(x = height, y = earn, color = as.factor(gender))) +
  geom_point(alpha = 0.1) +
  geom_line(aes(y = predict(mod5a))) +
  coord_cartesian(ylim = c(0, 100000))
```



## 交互项

然而，模型5的一个局限性，因为模型的结论从图形上看，是两条**平行的直线**：

1. 性别的影响对不同身高的人是相同的，
2. 或者，不管男性女性，收入随身高的增长是相同的。

虽然我们分组考虑不同性别的影响，但模型结论不一定符合现实情况。



为了扩展模型能力，允许预测因子之间相互影响，即需要考虑**交互项**。


$$
\begin{aligned}
Y_{i}=& \beta_{0}+\beta_{1}\textrm{height}_{i}+\beta_{2}\textrm{gender}_{i} \; +\\
      &{}\beta_{3}\textrm{height}_{i}\times\textrm{gender}_{i}+\epsilon_{i}\quad \textrm{where}\quad \epsilon_{i}\sim \textrm{N}(0,\sigma^2)
\end{aligned}
$$

```{r}
mod6 <- lm(earn ~ 1 + height + gender + height:gender, data = wages_gender)
```


```{r}
summary(mod6)
```




### 解释

为了方便理解，我们仍然分开来看

$$
\begin{aligned}
 \textrm{gender}=0: & \\
 \hat{y}_{i} &= -12166.97 + 564.51\textrm{height}_{i} \\
 \textrm{gender}=1: & \\
 \hat{Y}_{i} &= (-12166.97-30510.43)+(564.51 + 701.41)\textrm{height}_{i} 
\end{aligned}
$$



- 对于女性，height增长1个单位，引起earn的增长`564.5102`
- 对于男性，height增长1个单位，引起earn的增长`564.5102 + 701.4065 = 1265.92` 



两条拟合直线，不同的截距和不同的斜率。


```{r}
wages %>%
  ggplot(aes(x = height, y = earn, color = sex)) +
  geom_point(alpha = 0.1) +
  geom_line(aes(y = predict(mod6))) +
  coord_cartesian(ylim = c(0, 100000))
```


对于男性和女性，截距和系数都不同，因此这种情形**等价于**，按照sex分成两组，男性算男性的斜率，女性算女性的斜率（是不是似曾相识？）

```{r}
wages %>% 
  ggplot(aes(x = height, y = earn, color = sex)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm", se = FALSE) +
  coord_cartesian(ylim = c(0, 100000))
```



